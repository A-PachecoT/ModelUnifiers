{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4DFtQNDYao1"
      },
      "source": [
        "# Intro to TorchRec\n",
        "\n",
        "Frequently, when building recommendation systems, we want to represent entities like products or pages with embeddings. For example, see Meta AI's [Deep learning recommendation model](https://arxiv.org/abs/1906.00091), or DLRM. As the number of entities grow, the size of the embedding tables can exceed a single GPU’s memory. A common practice is to shard the embedding table across devices, a type of model parallelism. To that end, **TorchRec introduces its primary API called [`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel), or DMP. Like pytorch’s DistributedDataParallel, DMP wraps a model to enable distributed training.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBgIy9eYYx35"
      },
      "source": [
        "## **Installation**\n",
        "Requirements:\n",
        "- python >= 3.7\n",
        "\n",
        "We highly recommend CUDA when using TorchRec. If using CUDA:\n",
        "- cuda >= 11.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BB2K68OYUJ_t",
        "outputId": "156e2d86-37d8-4ea6-b106-57e6473d0759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-29 02:37:27--  https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
            "Resolving repo.anaconda.com (repo.anaconda.com)... 104.16.131.3, 104.16.130.3, 2606:4700::6810:8203, ...\n",
            "Connecting to repo.anaconda.com (repo.anaconda.com)|104.16.131.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 90040905 (86M) [application/x-sh]\n",
            "Saving to: ‘Miniconda3-py37_4.9.2-Linux-x86_64.sh’\n",
            "\n",
            "Miniconda3-py37_4.9 100%[===================>]  85.87M   201MB/s    in 0.4s    \n",
            "\n",
            "2023-07-29 02:37:28 (201 MB/s) - ‘Miniconda3-py37_4.9.2-Linux-x86_64.sh’ saved [90040905/90040905]\n",
            "\n",
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - brotlipy==0.7.0=py37h27cfd23_1003\n",
            "    - ca-certificates==2020.10.14=0\n",
            "    - certifi==2020.6.20=pyhd3eb1b0_3\n",
            "    - cffi==1.14.3=py37h261ae71_2\n",
            "    - chardet==3.0.4=py37h06a4308_1003\n",
            "    - conda-package-handling==1.7.2=py37h03888b9_0\n",
            "    - conda==4.9.2=py37h06a4308_0\n",
            "    - cryptography==3.2.1=py37h3c74f83_1\n",
            "    - idna==2.10=py_0\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20191231=h14c3975_1\n",
            "    - libffi==3.3=he6710b0_2\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_1\n",
            "    - openssl==1.1.1h=h7b6447c_0\n",
            "    - pip==20.2.4=py37h06a4308_0\n",
            "    - pycosat==0.6.3=py37h27cfd23_0\n",
            "    - pycparser==2.20=py_2\n",
            "    - pyopenssl==19.1.0=pyhd3eb1b0_1\n",
            "    - pysocks==1.7.1=py37_1\n",
            "    - python==3.7.9=h7579374_0\n",
            "    - readline==8.0=h7b6447c_0\n",
            "    - requests==2.24.0=py_0\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_1\n",
            "    - setuptools==50.3.1=py37h06a4308_1\n",
            "    - six==1.15.0=py37h06a4308_0\n",
            "    - sqlite==3.33.0=h62c20be_0\n",
            "    - tk==8.6.10=hbc83047_0\n",
            "    - tqdm==4.51.0=pyhd3eb1b0_0\n",
            "    - urllib3==1.25.11=py_0\n",
            "    - wheel==0.35.1=pyhd3eb1b0_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.2.5=h7b6447c_0\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  brotlipy           pkgs/main/linux-64::brotlipy-0.7.0-py37h27cfd23_1003\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.10.14-0\n",
            "  certifi            pkgs/main/noarch::certifi-2020.6.20-pyhd3eb1b0_3\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.3-py37h261ae71_2\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37h06a4308_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.9.2-py37h06a4308_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.7.2-py37h03888b9_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-3.2.1-py37h3c74f83_1\n",
            "  idna               pkgs/main/noarch::idna-2.10-py_0\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20191231-h14c3975_1\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_2\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1h-h7b6447c_0\n",
            "  pip                pkgs/main/linux-64::pip-20.2.4-py37h06a4308_0\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h27cfd23_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.20-py_2\n",
            "  pyopenssl          pkgs/main/noarch::pyopenssl-19.1.0-pyhd3eb1b0_1\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_1\n",
            "  python             pkgs/main/linux-64::python-3.7.9-h7579374_0\n",
            "  readline           pkgs/main/linux-64::readline-8.0-h7b6447c_0\n",
            "  requests           pkgs/main/noarch::requests-2.24.0-py_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_1\n",
            "  setuptools         pkgs/main/linux-64::setuptools-50.3.1-py37h06a4308_1\n",
            "  six                pkgs/main/linux-64::six-1.15.0-py37h06a4308_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.33.0-h62c20be_0\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.10-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.51.0-pyhd3eb1b0_0\n",
            "  urllib3            pkgs/main/noarch::urllib3-1.25.11-py_0\n",
            "  wheel              pkgs/main/noarch::wheel-0.35.1-pyhd3eb1b0_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.2.5-h7b6447c_0\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n"
          ]
        }
      ],
      "source": [
        "# install conda to make installying pytorch with cudatoolkit 11.3 easier.\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-py37_4.9.2-Linux-x86_64.sh\n",
        "!bash ./Miniconda3-py37_4.9.2-Linux-x86_64.sh -b -f -p /usr/local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sFYvP95xaAER",
        "outputId": "38fa604c-339a-4448-9df0-458d4c87ddfb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\bfailed with initial frozen solve. Retrying with flexible solve.\n",
            "Solving environment: \\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bfailed with repodata from current_repodata.json, will retry with next repodata source.\n",
            "Collecting package metadata (repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Solving environment: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch\n",
            "    - pytorch-cuda=11.6\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-5.1          |            1_gnu          21 KB\n",
            "    _pytorch_select-0.1        |            cpu_0           3 KB\n",
            "    blas-1.0                   |              mkl           6 KB\n",
            "    ca-certificates-2023.05.30 |       h06a4308_0         120 KB\n",
            "    certifi-2022.12.7          |   py37h06a4308_0         150 KB\n",
            "    conda-23.1.0               |   py37h06a4308_0         937 KB\n",
            "    cuda-cudart-11.6.55        |       he381448_0         194 KB  nvidia\n",
            "    cuda-cupti-11.6.124        |       h86345e5_0        22.1 MB  nvidia\n",
            "    cuda-libraries-11.6.1      |                0           1 KB  nvidia\n",
            "    cuda-nvrtc-11.6.124        |       h020bade_0        17.1 MB  nvidia\n",
            "    cuda-nvtx-11.6.124         |       h0630a44_0          58 KB  nvidia\n",
            "    cuda-runtime-11.6.1        |                0           1 KB  nvidia\n",
            "    flit-core-3.6.0            |     pyhd3eb1b0_0          42 KB\n",
            "    importlib-metadata-4.11.3  |   py37h06a4308_0          40 KB\n",
            "    importlib_metadata-4.11.3  |       hd3eb1b0_0          12 KB\n",
            "    intel-openmp-2019.4        |              243         729 KB\n",
            "    libcublas-11.9.2.110       |       h5e84587_0       300.8 MB  nvidia\n",
            "    libcufft-10.7.1.112        |       hf425ae0_0        93.6 MB  nvidia\n",
            "    libcufile-1.7.0.149        |                0        1021 KB  nvidia\n",
            "    libcurand-10.3.3.53        |                0        51.6 MB  nvidia\n",
            "    libcusolver-11.3.4.124     |       h33c3c4e_0        87.0 MB  nvidia\n",
            "    libcusparse-11.7.2.124     |       h7538f96_0       160.9 MB  nvidia\n",
            "    libgomp-11.2.0             |       h1234567_1         474 KB\n",
            "    libmklml-2019.0.5          |       h06a4308_0        22.1 MB\n",
            "    libnpp-11.6.3.124          |       hd2722f0_0       118.4 MB  nvidia\n",
            "    libnvjpeg-11.6.2.124       |       hd473ad6_0         2.3 MB  nvidia\n",
            "    mkl-2020.2                 |              256       138.3 MB\n",
            "    mkl-service-2.3.0          |   py37he8ac12f_0          52 KB\n",
            "    mkl_fft-1.3.0              |   py37h54f3939_0         167 KB\n",
            "    mkl_random-1.1.1           |   py37h0573a6f_0         322 KB\n",
            "    ninja-1.10.2               |       h06a4308_5           8 KB\n",
            "    ninja-base-1.10.2          |       hd09550d_5         109 KB\n",
            "    numpy-1.19.2               |   py37h54aff64_0          22 KB\n",
            "    numpy-base-1.19.2          |   py37hfa32c7d_0         4.1 MB\n",
            "    openssl-1.1.1u             |       h7f8727e_0         3.7 MB\n",
            "    pluggy-1.0.0               |   py37h06a4308_1          29 KB\n",
            "    pytorch-1.8.1              |cpu_py37h60491be_0        35.2 MB\n",
            "    pytorch-cuda-11.6          |       hfa5cb2e_3           7 KB  pytorch-nightly\n",
            "    ruamel.yaml-0.16.12        |   py37h7b6447c_1         173 KB\n",
            "    ruamel.yaml.clib-0.2.6     |   py37h7f8727e_0         133 KB\n",
            "    toolz-0.12.0               |   py37h06a4308_0         104 KB\n",
            "    typing-extensions-4.4.0    |   py37h06a4308_0           8 KB\n",
            "    typing_extensions-4.4.0    |   py37h06a4308_0          45 KB\n",
            "    zipp-3.11.0                |   py37h06a4308_0          19 KB\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        1.04 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _openmp_mutex      pkgs/main/linux-64::_openmp_mutex-5.1-1_gnu\n",
            "  _pytorch_select    pkgs/main/linux-64::_pytorch_select-0.1-cpu_0\n",
            "  blas               pkgs/main/linux-64::blas-1.0-mkl\n",
            "  cuda-cudart        nvidia/linux-64::cuda-cudart-11.6.55-he381448_0\n",
            "  cuda-cupti         nvidia/linux-64::cuda-cupti-11.6.124-h86345e5_0\n",
            "  cuda-libraries     nvidia/linux-64::cuda-libraries-11.6.1-0\n",
            "  cuda-nvrtc         nvidia/linux-64::cuda-nvrtc-11.6.124-h020bade_0\n",
            "  cuda-nvtx          nvidia/linux-64::cuda-nvtx-11.6.124-h0630a44_0\n",
            "  cuda-runtime       nvidia/linux-64::cuda-runtime-11.6.1-0\n",
            "  flit-core          pkgs/main/noarch::flit-core-3.6.0-pyhd3eb1b0_0\n",
            "  importlib-metadata pkgs/main/linux-64::importlib-metadata-4.11.3-py37h06a4308_0\n",
            "  importlib_metadata pkgs/main/noarch::importlib_metadata-4.11.3-hd3eb1b0_0\n",
            "  intel-openmp       pkgs/main/linux-64::intel-openmp-2019.4-243\n",
            "  libcublas          nvidia/linux-64::libcublas-11.9.2.110-h5e84587_0\n",
            "  libcufft           nvidia/linux-64::libcufft-10.7.1.112-hf425ae0_0\n",
            "  libcufile          nvidia/linux-64::libcufile-1.7.0.149-0\n",
            "  libcurand          nvidia/linux-64::libcurand-10.3.3.53-0\n",
            "  libcusolver        nvidia/linux-64::libcusolver-11.3.4.124-h33c3c4e_0\n",
            "  libcusparse        nvidia/linux-64::libcusparse-11.7.2.124-h7538f96_0\n",
            "  libgomp            pkgs/main/linux-64::libgomp-11.2.0-h1234567_1\n",
            "  libmklml           pkgs/main/linux-64::libmklml-2019.0.5-h06a4308_0\n",
            "  libnpp             nvidia/linux-64::libnpp-11.6.3.124-hd2722f0_0\n",
            "  libnvjpeg          nvidia/linux-64::libnvjpeg-11.6.2.124-hd473ad6_0\n",
            "  mkl                pkgs/main/linux-64::mkl-2020.2-256\n",
            "  mkl-service        pkgs/main/linux-64::mkl-service-2.3.0-py37he8ac12f_0\n",
            "  mkl_fft            pkgs/main/linux-64::mkl_fft-1.3.0-py37h54f3939_0\n",
            "  mkl_random         pkgs/main/linux-64::mkl_random-1.1.1-py37h0573a6f_0\n",
            "  ninja              pkgs/main/linux-64::ninja-1.10.2-h06a4308_5\n",
            "  ninja-base         pkgs/main/linux-64::ninja-base-1.10.2-hd09550d_5\n",
            "  numpy              pkgs/main/linux-64::numpy-1.19.2-py37h54aff64_0\n",
            "  numpy-base         pkgs/main/linux-64::numpy-base-1.19.2-py37hfa32c7d_0\n",
            "  pluggy             pkgs/main/linux-64::pluggy-1.0.0-py37h06a4308_1\n",
            "  pytorch            pkgs/main/linux-64::pytorch-1.8.1-cpu_py37h60491be_0\n",
            "  pytorch-cuda       pytorch-nightly/linux-64::pytorch-cuda-11.6-hfa5cb2e_3\n",
            "  ruamel.yaml        pkgs/main/linux-64::ruamel.yaml-0.16.12-py37h7b6447c_1\n",
            "  ruamel.yaml.clib   pkgs/main/linux-64::ruamel.yaml.clib-0.2.6-py37h7f8727e_0\n",
            "  toolz              pkgs/main/linux-64::toolz-0.12.0-py37h06a4308_0\n",
            "  typing-extensions  pkgs/main/linux-64::typing-extensions-4.4.0-py37h06a4308_0\n",
            "  typing_extensions  pkgs/main/linux-64::typing_extensions-4.4.0-py37h06a4308_0\n",
            "  zipp               pkgs/main/linux-64::zipp-3.11.0-py37h06a4308_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                              2020.10.14-0 --> 2023.05.30-h06a4308_0\n",
            "  certifi            pkgs/main/noarch::certifi-2020.6.20-p~ --> pkgs/main/linux-64::certifi-2022.12.7-py37h06a4308_0\n",
            "  conda                                4.9.2-py37h06a4308_0 --> 23.1.0-py37h06a4308_0\n",
            "  openssl                                 1.1.1h-h7b6447c_0 --> 1.1.1u-h7f8727e_0\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "importlib-metadata-4 | 40 KB     | : 100% 1.0/1 [00:00<00:00,  4.91it/s]\n",
            "cuda-nvtx-11.6.124   | 58 KB     | : 100% 1.0/1 [00:00<00:00,  9.36it/s]\n",
            "ruamel.yaml.clib-0.2 | 133 KB    | : 100% 1.0/1 [00:00<00:00,  5.64it/s]\n",
            "toolz-0.12.0         | 104 KB    | : 100% 1.0/1 [00:00<00:00,  6.04it/s]\n",
            "pluggy-1.0.0         | 29 KB     | : 100% 1.0/1 [00:00<00:00,  6.64it/s]\n",
            "_pytorch_select-0.1  | 3 KB      | : 100% 1.0/1 [00:00<00:00,  6.70it/s]\n",
            "importlib_metadata-4 | 12 KB     | : 100% 1.0/1 [00:00<00:00,  7.06it/s]\n",
            "libcublas-11.9.2.110 | 300.8 MB  | : 100% 1.0/1 [00:40<00:00, 40.17s/it]              \n",
            "ninja-1.10.2         | 8 KB      | : 100% 1.0/1 [00:00<00:00,  6.70it/s]\n",
            "mkl-service-2.3.0    | 52 KB     | : 100% 1.0/1 [00:00<00:00,  6.93it/s]\n",
            "libcurand-10.3.3.53  | 51.6 MB   | : 100% 1.0/1 [00:07<00:00,  7.41s/it]               \n",
            "numpy-base-1.19.2    | 4.1 MB    | : 100% 1.0/1 [00:00<00:00,  2.93it/s]\n",
            "cuda-nvrtc-11.6.124  | 17.1 MB   | : 100% 1.0/1 [00:02<00:00,  2.61s/it]\n",
            "ruamel.yaml-0.16.12  | 173 KB    | : 100% 1.0/1 [00:00<00:00,  6.51it/s]\n",
            "certifi-2022.12.7    | 150 KB    | : 100% 1.0/1 [00:00<00:00,  6.72it/s]\n",
            "blas-1.0             | 6 KB      | : 100% 1.0/1 [00:00<00:00,  7.24it/s]\n",
            "mkl_fft-1.3.0        | 167 KB    | : 100% 1.0/1 [00:00<00:00,  6.48it/s]\n",
            "conda-23.1.0         | 937 KB    | : 100% 1.0/1 [00:00<00:00,  4.56it/s]\n",
            "typing_extensions-4. | 45 KB     | : 100% 1.0/1 [00:00<00:00,  6.89it/s]\n",
            "zipp-3.11.0          | 19 KB     | : 100% 1.0/1 [00:00<00:00,  6.95it/s]\n",
            "cuda-libraries-11.6. | 1 KB      | : 100% 1.0/1 [00:00<00:00, 22.38it/s]\n",
            "pytorch-cuda-11.6    | 7 KB      | : 100% 1.0/1 [00:00<00:00,  1.51it/s]\n",
            "libmklml-2019.0.5    | 22.1 MB   | : 100% 1.0/1 [00:01<00:00,  1.07s/it]\n",
            "libcusparse-11.7.2.1 | 160.9 MB  | : 100% 1.0/1 [00:21<00:00, 21.89s/it]               \n",
            "mkl_random-1.1.1     | 322 KB    | : 100% 1.0/1 [00:00<00:00,  5.36it/s]\n",
            "pytorch-1.8.1        | 35.2 MB   | : 100% 1.0/1 [00:03<00:00,  3.06s/it]              \n",
            "mkl-2020.2           | 138.3 MB  | : 100% 1.0/1 [00:05<00:00,  5.75s/it]\n",
            "flit-core-3.6.0      | 42 KB     | : 100% 1.0/1 [00:00<00:00,  6.31it/s]\n",
            "libnpp-11.6.3.124    | 118.4 MB  | : 100% 1.0/1 [00:16<00:00, 16.56s/it]               \n",
            "intel-openmp-2019.4  | 729 KB    | : 100% 1.0/1 [00:00<00:00,  5.86it/s]\n",
            "_openmp_mutex-5.1    | 21 KB     | : 100% 1.0/1 [00:00<00:00,  6.81it/s]\n",
            "cuda-runtime-11.6.1  | 1 KB      | : 100% 1.0/1 [00:00<00:00, 22.30it/s]\n",
            "ca-certificates-2023 | 120 KB    | : 100% 1.0/1 [00:00<00:00,  7.01it/s]\n",
            "ninja-base-1.10.2    | 109 KB    | : 100% 1.0/1 [00:00<00:00,  6.85it/s]\n",
            "typing-extensions-4. | 8 KB      | : 100% 1.0/1 [00:00<00:00,  7.07it/s]\n",
            "libcusolver-11.3.4.1 | 87.0 MB   | : 100% 1.0/1 [00:18<00:00, 18.59s/it]               \n",
            "libnvjpeg-11.6.2.124 | 2.3 MB    | : 100% 1.0/1 [00:00<00:00,  2.62it/s]\n",
            "libgomp-11.2.0       | 474 KB    | : 100% 1.0/1 [00:00<00:00,  5.19it/s]\n",
            "libcufft-10.7.1.112  | 93.6 MB   | : 100% 1.0/1 [00:13<00:00, 13.25s/it]               \n",
            "numpy-1.19.2         | 22 KB     | : 100% 1.0/1 [00:00<00:00,  6.65it/s]\n",
            "openssl-1.1.1u       | 3.7 MB    | : 100% 1.0/1 [00:00<00:00,  4.34it/s]\n",
            "libcufile-1.7.0.149  | 1021 KB   | : 100% 1.0/1 [00:00<00:00,  4.95it/s]\n",
            "cuda-cupti-11.6.124  | 22.1 MB   | : 100% 1.0/1 [00:03<00:00,  3.64s/it]\n",
            "cuda-cudart-11.6.55  | 194 KB    | : 100% 1.0/1 [00:00<00:00, 14.36it/s]\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\bdone\n",
            "Verifying transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "Executing transaction: / \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n"
          ]
        }
      ],
      "source": [
        "# install pytorch with cudatoolkit 11.6\n",
        "!conda install pytorch pytorch-cuda=11.6 -c pytorch-nightly -c nvidia -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iY7Uv11mJYK"
      },
      "source": [
        "Installing TorchRec will also install [FBGEMM](https://github.com/pytorch/fbgemm), a collection of CUDA kernels and GPU enabled operations to run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tUnIw-ZREQJy",
        "outputId": "f8d84b41-8eed-4360-f6d0-4f64f746204e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchrec-nightly\n",
            "  Downloading torchrec_nightly-2023.1.26-py37-none-any.whl (322 kB)\n",
            "\u001b[K     |████████████████████████████████| 322 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[K     |████████████████████████████████| 519 kB 48.0 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 39.1 MB/s \n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement fbgemm-gpu-nightly (from torchrec-nightly) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for fbgemm-gpu-nightly (from torchrec-nightly)\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install torchrec\n",
        "!pip3 install torchrec-nightly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6EHgotRXFQh"
      },
      "source": [
        "The following steps are needed for the Colab runtime to detect the added shared libraries. The runtime searches for shared libraries in /usr/lib, so we copy over the libraries which were installed in /usr/local/lib/. **This is a very necessary step, only in the colab runtime**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_P45pDteRcWj"
      },
      "outputs": [],
      "source": [
        "!cp /usr/local/lib/lib* /usr/lib/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5_X2WOAYG3c"
      },
      "source": [
        "\\**Restart your runtime at this point for the newly installed packages to be seen.** Run the step below immediately after restarting so that python knows where to look for packages. **Always run this step after restarting the runtime.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8cktNrh8R9rC"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path = ['', '/env/python', '/usr/local/lib/python37.zip', '/usr/local/lib/python3.7', '/usr/local/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/site-packages']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWBOrwVSnrNE"
      },
      "source": [
        "## **Overview**\n",
        "This tutorial will cover three pieces of TorchRec - the `nn.module` `EmbeddingBagCollection`, the `DistributedModelParallel` API, and the datastructure `KeyedJaggedTensor`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udsN6PlUo1zF"
      },
      "source": [
        "### Distributed Setup\n",
        "We setup our environment with torch.distributed. For more info on distributed, see this [tutorial](https://pytorch.org/tutorials/beginner/dist_overview.html)\n",
        "\n",
        "Here, we use one rank (the colab process) corresponding to our 1 colab GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4-v17rxkopQw",
        "outputId": "30faf84b-82d8-4228-bd35-42e098e1456c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-f75f32cb10bb>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchrec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistributed\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# The __file__ check only works for Python 3.7 and above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_C_for_compiled_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__file__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         raise ImportError(textwrap.dedent('''\n\u001b[0m\u001b[1;32m    214\u001b[0m             \u001b[0mFailed\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mC\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mIt\u001b[0m \u001b[0mappears\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mfolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: Failed to load PyTorch C extensions:\n    It appears that PyTorch has loaded the `torch/_C` folder\n    of the PyTorch repository rather than the C extensions which\n    are expected in the `torch._C` namespace. This can occur when\n    using the `install` workflow. e.g.\n        $ python setup.py install && python -c \"import torch\"\n\n    This error can generally be solved using the `develop` workflow\n        $ python setup.py develop && python -c \"import torch\"  # This should succeed\n    or by running Python from a different directory.",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchrec\n",
        "import torch.distributed as dist\n",
        "\n",
        "os.environ[\"RANK\"] = \"0\"\n",
        "os.environ[\"WORLD_SIZE\"] = \"1\"\n",
        "os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
        "os.environ[\"MASTER_PORT\"] = \"29500\"\n",
        "\n",
        "# Note - you will need a V100 or A100 to run tutorial as as!\n",
        "# If using an older GPU (such as colab free K80),\n",
        "# you will need to compile fbgemm with the appripriate CUDA architecture\n",
        "# or run with \"gloo\" on CPUs\n",
        "dist.init_process_group(backend=\"nccl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdSUWBRxoP8R"
      },
      "source": [
        "### From EmbeddingBag to EmbeddingBagCollection\n",
        "Pytorch represents embeddings through [`torch.nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) and [`torch.nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html). EmbeddingBag is a pooled version of Embedding.\n",
        "\n",
        "TorchRec extends these modules by creating collections of embeddings. We will use [`EmbeddingBagCollection`](https://pytorch.org/torchrec/torchrec.modules.html#torchrec.modules.embedding_modules.EmbeddingBagCollection) to represent a group of EmbeddingBags.\n",
        "\n",
        "Here, we create an EmbeddingBagCollection (EBC) with two embedding bags. Each table, `product_table` and `user_table`, is represented by 64 dimension embedding of size 4096. Note how we initially allocate the EBC on device \"meta\". This will tell EBC to not allocate memory yet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iz_GZDp_oQ19"
      },
      "outputs": [],
      "source": [
        "ebc = torchrec.EmbeddingBagCollection(\n",
        "    device=\"meta\",\n",
        "    tables=[\n",
        "        torchrec.EmbeddingBagConfig(\n",
        "            name=\"product_table\",\n",
        "            embedding_dim=64,\n",
        "            num_embeddings=4096,\n",
        "            feature_names=[\"product\"],\n",
        "            pooling=torchrec.PoolingType.SUM,\n",
        "        ),\n",
        "        torchrec.EmbeddingBagConfig(\n",
        "            name=\"user_table\",\n",
        "            embedding_dim=64,\n",
        "            num_embeddings=4096,\n",
        "            feature_names=[\"user\"],\n",
        "            pooling=torchrec.PoolingType.SUM,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jtv2qEhgQww"
      },
      "source": [
        "# FBGEMM Optimizations - Batching and Fusion\n",
        "\n",
        "TorchRec provides abstractions over [FBGEMM](https://github.com/pytorch/FBGEMM/tree/main/fbgemm_gpu) kernels that provide efficient implementations of the canonical nn.EmbeddingBags. Two of the optimizations that can be done are\n",
        "\n",
        "* Table batching, which allows you to look up multiple embeddings with one kernel call.\n",
        "* Optimizer Fusion, which allows the module to update itself given the canonical pytorch optimizers and arguments.\n",
        "\n",
        "This can be accessed by using the [fuse_embedding_optimizer](https://github.com/pytorch/torchrec/blob/main/torchrec/modules/fused_embedding_modules.py#L271) wrapper, which will replace embedding modules with their batched and fused counter parts. You can also directly use these efficient counterparts, take a look at torchrec.modules.fused_embedding_modules.\n",
        "\n",
        "To quantitatively see the performance gains of this, see our [benchmarks](https://github.com/pytorch/torchrec/blob/main/benchmarks/README.md).\n",
        "\n",
        "Note that this step is optional - the following steps can also be applied to the non-optimizer EmbeddingBagCollection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qa0KmLuugQwx"
      },
      "outputs": [],
      "source": [
        "from torchrec.optim.apply_optimizer_in_backward import apply_optimizer_in_backward\n",
        "\n",
        "apply_optimizer_in_backward(\n",
        "    optimizer_class=torch.optim.SGD,\n",
        "    params=ebc.parameters(),\n",
        "    optimizer_kwargs={\"lr\": 0.02},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7m0_ssVLFQEH"
      },
      "source": [
        "### DistributedModelParallel\n",
        "Now, we’re ready to wrap our model with [`DistributedModelParallel`](https://pytorch.org/torchrec/torchrec.distributed.html#torchrec.distributed.model_parallel.DistributedModelParallel) (DMP). Instantiating DMP will:\n",
        "\n",
        "1. Decide how to shard the model. DMP will collect the available ‘sharders’ and come up with a ‘plan’ of the optimal way to shard the embedding table(s) (i.e, the EmbeddingBagCollection)\n",
        "2. Actually shard the model. This includes allocating memory for each embedding table on the appropriate device(s).\n",
        "\n",
        "In this toy example, since we have two EmbeddingTables and one GPU, TorchRec will place both on the single GPU.\n",
        "\n",
        "To learn more about sharding, see our [sharding tutorial](https://pytorch.org/tutorials/advanced/sharding.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arW0Jf6qEl-h"
      },
      "outputs": [],
      "source": [
        "model = torchrec.distributed.DistributedModelParallel(ebc, device=torch.device(\"cuda\"))\n",
        "print(model)\n",
        "print(model.plan)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7djgqRegQw0"
      },
      "source": [
        "## Sharders and Quantized Comms\n",
        "\n",
        "By default, DistributedModelParallel will identify which Sharder to use for your embedding module. In the above case, it creates a default EmbeddingBagCollectionSharder.\n",
        "\n",
        "However, you may also specify your own Sharder; by doing so, you can set additional sharding parameters. For example, you can specify the quantized/mixed precision config.\n",
        "\n",
        "Applying quantization and mixed precision to collective calls as part of distributed training is often used to increase the model's training throughput, while at the same time, not significantly sacrificing model quality.\n",
        "\n",
        "TorchRec provides helper functions to construct communication codecs, based on the [FBGEMM Qcomm library](https://github.com/pytorch/FBGEMM/blob/main/fbgemm_gpu/fbgemm_gpu/quantize_comm.py).\n",
        "\n",
        "Below, we create a sharder that uses FP16 mixed precision for the forward pass (when passing embedding tensors around in a collective call, first cast them to FP16). And similarly, for the backwards pass, cast tensors within collective calls to BF16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3Rpi2VKgQw1"
      },
      "outputs": [],
      "source": [
        "from torchrec.distributed.fbgemm_qcomm_codec import get_qcomm_codecs_registry, QCommsConfig, CommType\n",
        "from torchrec.distributed.embeddingbag import EmbeddingBagCollectionSharder\n",
        "\n",
        "sharder = EmbeddingBagCollectionSharder(\n",
        "    qcomm_codecs_registry=get_qcomm_codecs_registry(\n",
        "            qcomms_config=QCommsConfig(\n",
        "                forward_precision=CommType.FP16,\n",
        "                backward_precision=CommType.BF16,\n",
        "            )\n",
        "        )\n",
        ")\n",
        "model = torchrec.distributed.DistributedModelParallel(ebc, sharders=[sharder], device=torch.device(\"cuda\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ovgAgthgQw2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slQSqiIxQHVW"
      },
      "source": [
        "### Query vanilla nn.EmbeddingBag with input and offsets\n",
        "\n",
        "We query [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) and [`nn.EmbeddingBag`](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html) with `input` and `offsets`. Input is a 1-D tensor containing the lookup values. Offsets is a 1-D tensor where the sequence is a cumulative sum of the number of values to pool per example.\n",
        "\n",
        "Let's look at an example, recreating the product EmbeddingBag above\n",
        "\n",
        "```\n",
        "|------------|\n",
        "| product ID |\n",
        "|------------|\n",
        "| [101, 202] |\n",
        "| []         |\n",
        "| [303]      |\n",
        "|------------|\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T_SExFDBqHS"
      },
      "outputs": [],
      "source": [
        "product_eb = torch.nn.EmbeddingBag(4096, 64)\n",
        "product_eb(input=torch.tensor([101, 202, 303]), offsets=torch.tensor([0, 2, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxFOoBnZCbRX"
      },
      "source": [
        "### Representing minibatches with KeyedJaggedTensor\n",
        "\n",
        "We need an efficient representation of multiple examples of an arbitrary number of entity IDs per feature per example. In order to enable this \"jagged\" representation, we use the TorchRec datastructure [`KeyedJaggedTensor`](https://pytorch.org/torchrec/torchrec.sparse.html#torchrec.sparse.jagged_tensor.JaggedTensor) (KJT).\n",
        "\n",
        "Let's take a look at **how to lookup a collection of two embedding bags**, \"product\" and \"user\".  Assume the minibatch is made up of three examples for three users. The first of which has two product IDs, the second with none, and the third with one product ID.\n",
        "\n",
        "```\n",
        "|------------|------------|\n",
        "| product ID | user ID    |\n",
        "|------------|------------|\n",
        "| [101, 202] | [404]      |\n",
        "| []         | [505]      |\n",
        "| [303]      | [606]      |\n",
        "|------------|------------|\n",
        "```\n",
        "\n",
        "The query should be:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKxjPYbpDY3k"
      },
      "outputs": [],
      "source": [
        "mb = torchrec.KeyedJaggedTensor(\n",
        "    keys = [\"product\", \"user\"],\n",
        "    values = torch.tensor([101, 202, 303, 404, 505, 606]).cuda(),\n",
        "    lengths = torch.tensor([2, 0, 1, 1, 1, 1], dtype=torch.int64).cuda(),\n",
        ")\n",
        "\n",
        "print(mb.to(torch.device(\"cpu\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co1Tb5RQ-J5a"
      },
      "source": [
        "Note that the KJT batch size is `batch_size = len(lengths)//len(keys)`. **In the above example, batch_size is 3.**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjP4ctxqnmsU"
      },
      "source": [
        "### Putting it all together, querying our distributed model with a KJT minibatch\n",
        "Finally, we can query our model using our minibatch of products and users.\n",
        "\n",
        "The resulting lookup will contain a KeyedTensor, where each key (or feature) contains a 2D tensor of size 3x64 (batch_size x embedding_dim)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TmnV3iH4IXn8"
      },
      "outputs": [],
      "source": [
        "pooled_embeddings = model(mb).to_dict()\n",
        "print(\"product embeddings\", pooled_embeddings[\"product\"])\n",
        "print(\"user embeddings\", pooled_embeddings[\"user\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebXfh7oW9fHH"
      },
      "source": [
        "## More resources\n",
        "For more information, please see our [dlrm](https://github.com/facebookresearch/dlrm/tree/main/torchrec_dlrm/) example, which includes multinode training on the criteo terabyte dataset, using Meta’s [DLRM](https://arxiv.org/abs/1906.00091)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Torchrec Introduction.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "interpreter": {
      "hash": "d4204deb07d30e7517ec64733b2d65f24aff851b061e21418071854b06459363"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}